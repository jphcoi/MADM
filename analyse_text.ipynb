{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bamboolib\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FixedFormatter, FixedLocator\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from IPython.core.display import display, HTML\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7190f",
   "metadata": {},
   "source": [
    "## Analyse des segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aymeric =  \"/home/aymeric/python-scripts/anses_medialab/datas/\" #aymeric\n",
    "jp = '~/Dropbox/Mac/Desktop/CRD Anses/all3/' # Jean Philippe\n",
    "jp_index = '~/Dropbox/Mac/Desktop/CRD Anses/code/indexation_results/' # Jean Philippe index\n",
    "\n",
    "if 'aymeric' in current_directory:\n",
    "    path_base = aymeric\n",
    "    index=f\"{path_base}index_allall_domainsexhaustive.csv\"\n",
    "elif 'Mac' in current_directory:\n",
    "    path_base = jp\n",
    "    index=f'{jp_index}index_allall_domainsexhaustive.csv'\n",
    "elif 'd:/Projects' in current_directory:\n",
    "    path_base = \"d:/Projects/Medialab/\"\n",
    "    index=f\"{path_base}index_allall_domainsexhaustive.csv\"\n",
    "\n",
    "file_segmentation = f\"{path_base}segmentation_common_freq.csv\"\n",
    "data_file = path_base+'fb_tw_med_data.csv.gz'#,line_terminator='\\n',index=False)\n",
    "\n",
    "\n",
    "all_corpus = f\"{path_base}all_corpus_seg_with_sdhi_PI.csv\"\n",
    "cluster_qualification = f\"{path_base}cluster_qualification.csv\"\n",
    "\n",
    "#twittos = f\"{path_base}tweets_pesticides/500_first_twittos_on_pesticides.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_base = [\"index\", \"all_corpus\"]\n",
    "which_base = list_base[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfseg0 = pd.read_csv(file_segmentation, sep = \"\\t\")\n",
    "df_cluster = pd.read_csv(cluster_qualification, sep = \"\\t\")\n",
    "\n",
    "if which_base == \"all_corpus\":\n",
    "    df0 = pd.read_csv(all_corpus, sep = \"\\t\")\n",
    "else:\n",
    "    df_index = pd.read_csv(index, sep = \"\\t\")\n",
    "\n",
    "    df_index['date'] = pd.to_datetime(df_index['date'], infer_datetime_format=True)\n",
    "    df_index['yearmonth']=(df_index['date'].dt.strftime('%Y-%m'))\n",
    "    df_index['date'] = df_index['date'].dt.date\n",
    "    \n",
    "    segment = dfseg0[[\"yearmonth\", \"segm\", \"origin\"]]\n",
    "    df0 = df_index.merge(segment, how = \"inner\", on = [\"origin\", \"yearmonth\"])\n",
    "    df0[\"start_segment\"] = df0.groupby(['origin','segm'])[\"date\"].transform('min')\n",
    "    df0[\"end_segment\"] = df0.groupby(['origin','segm'])[\"date\"].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b866741",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Rename column\n",
    "dft = dft.rename(columns={'id': 'doc_id'})\n",
    "\n",
    "dft['date'] = pd.to_datetime(dft['date'], infer_datetime_format=True)\n",
    "\n",
    "dft['yearmonth']=(dft['date'].dt.strftime('%Y-%m'))\n",
    "dft['date'] = dft['date'].dt.date\n",
    "    \n",
    "segment = dfseg0[[\"yearmonth\", \"segm\", \"origin\"]]\n",
    "dft0 = dft.merge(segment, how = \"inner\", on = [\"origin\", \"yearmonth\"])\n",
    "dft0[\"start_segment\"] = dft0.groupby(['origin','segm'])[\"date\"].transform('min')\n",
    "dft0[\"end_segment\"] = dft0.groupby(['origin','segm'])[\"date\"].transform('max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfseg0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010b9f3",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62530b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, domain, segment):\n",
    "    print(domain)\n",
    "    dfseg = data[0]\n",
    "    dfseg1 = dfseg.loc[(dfseg[\"origin\"] == domain) & (dfseg[\"segm\"] == segment)]\n",
    "    \n",
    "    df = data[1]\n",
    "    df1 = df.loc[(df[\"origin\"] == domain) & (df[\"segm\"] == segment)]\n",
    "    \n",
    "    dft = data[2]\n",
    "    dft1 = dft.loc[(dft[\"origin\"] == domain) & (dft[\"segm\"] == segment)]\n",
    "    dft1['doc_id'] = dft1['doc_id'].astype('string')\n",
    "    \n",
    "    specific_terms = [x.strip() for x in dfseg1[\"top50_term_chi2_True\"].iloc[0].split(\",\")]\n",
    "\n",
    "    print(specific_terms)\n",
    "    df1.loc[df1[\"term\"].isin(specific_terms) == True, \"in_top50_term\"] = 1\n",
    "    df1.loc[df1[\"term\"].isin(specific_terms) == False, \"in_top50_term\"] = 0\n",
    "\n",
    "    \n",
    "    df2 = df1.drop_duplicates(subset = [\"doc_id\", \"term\"]).groupby(['doc_id', 'source', \"date\"]).agg(sum_of_top50_term =('in_top50_term', 'sum')).reset_index()\n",
    "    df2['doc_id'] = df2['doc_id'].astype('string')\n",
    "    df2 = df2.merge(dft1[[\"doc_id\", \"text\"]], how = \"inner\", on = [\"doc_id\"])\n",
    "    \n",
    "    return dfseg1, df1, dft1, df2, specific_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"facebook\", \"twitter\", \"media\"]\n",
    "segment = 1\n",
    "domain = domains[0]\n",
    "\n",
    "list_df = [dfseg0, df0, dft0]\n",
    "dfseg1, df1, dft1, df2, specific_terms = filter_data(list_df, domain = domain, segment = segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f64ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne = np.mean(df2[\"sum_of_top50_term\"]) \n",
    "df3 = df2.loc[df2[\"sum_of_top50_term\"] >= 1 ]\n",
    "df3 = df3.sort_values(by=['date'], ascending=[True])\n",
    "\n",
    "#df3 = df3.drop_duplicates(subset = \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd94bd",
   "metadata": {},
   "source": [
    "## Matrice de similarit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415de650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = df1.groupby(['term', 'doc_id']).agg(count_doc = (\"doc_id\", \"count\")).reset_index()\n",
    "df11=df11.merge(df1[['term','in_top50_term']].drop_duplicates(), on='term', how = 'left')\n",
    "df11=df11.loc[df11[\"in_top50_term\"] ==  1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dico_freq={}\n",
    "for l in df11.groupby('term'):\n",
    "    term=l[0]\n",
    "    #print (term)\n",
    "    dico_freq[term]={}\n",
    "    for d in l[1].groupby('doc_id'):\n",
    "        #print (d[0],d[1]['count_doc'].values[0])\n",
    "        dico_freq[term][d[0]]=d[1]['count_doc'].values[0]\n",
    "\n",
    "for t in dico_freq:\n",
    "    N=sum(dico_freq[t].values())\n",
    "    for d in dico_freq[t]:\n",
    "        dico_freq[t][d]=dico_freq[t][d]/N\n",
    "    \n",
    "                \n",
    "squared={}\n",
    "spec_words=list(dico_freq.keys())\n",
    "for w1 in spec_words:\n",
    "    s=0\n",
    "    for d1 in dico_freq[w1]:\n",
    "        #if d1 in spec_words[w2]:\n",
    "        s += dico_freq[w1][d1] * dico_freq[w1].get(d1,0)\n",
    "    squared[w1]=s\n",
    "\n",
    "similarity=[]\n",
    "spec_words=list(dico_freq.keys())\n",
    "for w1 in spec_words:\n",
    "    sim=[]\n",
    "    for w2 in spec_words:\n",
    "        s=0\n",
    "        for d1 in dico_freq[w1]:\n",
    "            #if d1 in spec_words[w2]:\n",
    "            s += dico_freq[w1][d1] * dico_freq[w2].get(d1,0)\n",
    "        sim.append(s/math.sqrt(squared[w1]*squared[w2]))\n",
    "    similarity.append(sim)\n",
    "\n",
    "#similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = [x for x in df11.term.unique()]\n",
    "print(list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b174a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "cmap_reversed = plt.cm.get_cmap('spring_r')\n",
    "#sns.clustermap(rho_terms)\n",
    "ax=sns.clustermap(similarity,figsize=(15,15), cmap = cmap_reversed, linewidths=2, linecolor='white', vmax = 0.8)#,labels=range(14))\n",
    "new_labels=[]\n",
    "labels_list= list_name\n",
    "for l in ax.ax_heatmap.axes.get_xticklabels():\n",
    "    l.set_text(labels_list[int(l.get_text())])\n",
    "    new_labels.append(l)\n",
    "ax.ax_heatmap.axes.set_yticklabels(new_labels,rotation=0)\n",
    "ax.ax_heatmap.axes.set_xticklabels(new_labels,rotation = 90)\n",
    "\n",
    "#ax.setp(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"clustermap/cm_{domain}_{segment}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211f3c1",
   "metadata": {},
   "source": [
    "## D√©termination du nombre de clusters avec kneed Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(15, 5))\n",
    "row_linkage = linkage(sp.distance.pdist(similarity), method='complete')# for x in (t.values, rho_terms.values.T))\n",
    "nbclust_max=20\n",
    "dfcost=pd.DataFrame()\n",
    "for nbclust in  range(1,nbclust_max):\n",
    "    cluster_idx = scipy.cluster.hierarchy.fcluster(row_linkage,nbclust,criterion='maxclust')\n",
    "    cluster = [y for y in cluster_idx]    \n",
    "    new_df = pd.DataFrame(data = {\"term\": list_name, \"cluster\": cluster})\n",
    "    new_df['nbseg']=nbclust\n",
    "    new_df_count = new_df.groupby(['cluster']).agg(term_size=('term', 'size')).reset_index()\n",
    "    new_df_count = new_df_count.loc[new_df_count['term_size'] == 1]\n",
    "    if len(new_df_count)>0:\n",
    "        break\n",
    "        \n",
    "    dfcost=dfcost.append(new_df)\n",
    "    \n",
    "#Liste des distances entre chaque paire de termes\n",
    "df_sim['v1']=df_sim.index\n",
    "df_sim_melt = df_sim.melt(id_vars=['v1'])\n",
    "df_sim_melt.columns=['v1', 'v2', 'sim']\n",
    "df_sim_melt['dist']=1-df_sim_melt['sim']\n",
    "df_sim_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90979c2b",
   "metadata": {},
   "source": [
    "## Distance moyenne entre √©l√©ments de chaque cluster (avec nombre de cluster compris entre 1 et 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_clust = pd.merge(df_sim_melt, dfcost, how='inner', left_on=['v1'], right_on=['term'])\n",
    "df_dist_clust = pd.merge(df_dist_clust, dfcost, how='inner', left_on=['v2'], right_on=['term'])\n",
    "df_dist_clust = df_dist_clust.loc[df_dist_clust['nbseg_x'] == df_dist_clust['nbseg_y']]\n",
    "df_dist_clust = df_dist_clust.loc[df_dist_clust['cluster_x'] == df_dist_clust['cluster_y']]\n",
    "#df_dist_clust['distsq']=df_dist_clust['dist']*df_dist_clust['dist']\n",
    "df_dist_clust_inertia = df_dist_clust.groupby(['nbseg_x', 'cluster_x' ,'v1']).agg(dist_mean=('dist', 'mean')).reset_index()\n",
    "df_dist_clust_inertia['distsq']=df_dist_clust_inertia['dist_mean'].pow(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e69cf0",
   "metadata": {},
   "source": [
    "## Somme des distances moyennes au carr√© intra-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279569be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_clust_inertia = df_dist_clust_inertia.groupby(['nbseg_x', 'cluster_x']).agg(dist_mean_sum=('distsq', 'sum')).reset_index()\n",
    "df_dist_clust_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb48d01",
   "metadata": {},
   "source": [
    "## Somme des distances moyennes au carr√© inter-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_clust_inertia = df_dist_clust_inertia.groupby(['nbseg_x']).agg(dist_mean_sum=('dist_mean_sum', 'sum')).reset_index()\n",
    "df_dist_clust_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553667fe",
   "metadata": {},
   "source": [
    "## Distribution de dist_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(df_dist_clust_inertia, x='nbseg_x', y='dist_mean_sum', width=800, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15827b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "x=df_dist_clust_inertia['nbseg_x'].values\n",
    "y=df_dist_clust_inertia['dist_mean_sum'].values\n",
    "kn = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "nb_segm_optimal=kn.knee\n",
    "print(nb_segm_optimal)\n",
    "\n",
    "dfcost = dfcost.loc[dfcost['nbseg'] == nb_segm_optimal]\n",
    "dfcost = dfcost.sort_values(by=['cluster'], ascending=[True])\n",
    "\n",
    "dfcost = dfcost.loc[dfcost['nbseg'] == nb_segm_optimal]\n",
    "dfcost = dfcost.sort_values(by=['cluster'], ascending=[True])\n",
    "dfcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp, scipy.cluster.hierarchy as hc\n",
    "\n",
    "fig = plt.figure(1, figsize=(10, 20))\n",
    "#th = 1.63\n",
    "row_linkage = linkage(sp.distance.pdist(similarity), method='complete')# for x in (t.values, rho_terms.values.T))\n",
    "dn = dendrogram(row_linkage, leaf_font_size= 12, labels=list_name, orientation = \"left\")\n",
    "#plt.axvline(x=th, c='grey', lw=1, linestyle='dashed')\n",
    "\n",
    "cluster_idx = scipy.cluster.hierarchy.fcluster(row_linkage,t= th,criterion=\"distance\")\n",
    "plt.savefig(f\"clustermap/dendogram_{domain}_{segment}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae48b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster = [y for y in cluster_idx]\n",
    "    \n",
    "new_df = pd.DataFrame(data = {\"term\": list_name, \"cluster\": cluster})\n",
    "new_df.sort_values(\"cluster\", ascending = True)\n",
    "\n",
    "dic_cluster = {1 : \"Alternatives aux pesticides\", \n",
    "               2 : \"Contre le monde de Monsanto\",  \n",
    "               3 : \"M√©decins\", \n",
    "               4 :\"R√©sidus chimiques\",\n",
    "               5 : \"Expertise et risque sanitaire\",\n",
    "               6 : \"Les syst√®mes agricoles en question\",\n",
    "               7 : \"Abeilles et n√©onicotino√Ødes\",\n",
    "               8 : \"OGM et herbicides\",\n",
    "               9 : \"G√©n√©rations futures\",\n",
    "               10 : \"Les r√©sidus de pesticides dans l'alimentation\"\n",
    "              }\n",
    "\n",
    "print(new_df.sort_values(\"cluster\", ascending = True))\n",
    "#print(len(dic_cluster), len(cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0353e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nombre_texts = len(df3[\"text\"])\n",
    "start_segment = df1[\"start_segment\"].iloc[1]#.strftime(\"%Y-%m-%d\")\n",
    "end_segment = df1[\"end_segment\"].iloc[1]#.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for i, text in enumerate(df3[\"text\"]):\n",
    "    doc_id = df3[\"doc_id\"].iloc[i]\n",
    "    list_term = [x for x in df1[\"term\"].loc[(df1[\"doc_id\"] == doc_id)].unique()]\n",
    "    term_to_search = ['victimes']\n",
    "    nb_term_to_search = [x for x in term_to_search if x in list_term]\n",
    "    \n",
    "    if len(nb_term_to_search) >= 0:\n",
    "        #Thematique = \"Tous contre Monsanto\"\n",
    "        #df1.loc[(df1[\"doc_id\"].str.contains(doc_id) == True), \"Subject\"] = Thematique\n",
    "        list_top50_term = [x for x in df1[\"term\"].loc[(df1[\"doc_id\"] == doc_id) & (df1[\"in_top50_term\"] == 1)].unique()]\n",
    "        list_top50_term_absent = set(specific_terms) - set(list_top50_term)\n",
    "        date = df3[\"date\"].iloc[i]#.strftime(\"%Y-%m-%d\")\n",
    "        auteur = df3[\"source\"].iloc[i]\n",
    "        display(HTML(f\"<h2 style='text-align:center;'> Texte {doc_id} ({i+1} sur {nombre_texts})</h2>\"))\n",
    "        display(HTML(f\"<strong>Segment :</strong> {segment} ({start_segment} ; {end_segment})\"))\n",
    "        display(HTML(f\"<strong>Date :</strong> {date}\"))\n",
    "        display(HTML(f\"<strong>Auteur :</strong> {auteur} \\n\"))\n",
    "        print(f\"Termes index√©s : {list_term} \\n\")\n",
    "        print(f\"Termes sp√©cifiques pr√©sents : {list_top50_term} \\n\")\n",
    "        print(f\"Termes sp√©cifiques absents : {list_top50_term_absent} \\n\")\n",
    "        #display(HTML(f\"<strong>Th√©matique :</strong> {Thematique} \\n\"))\n",
    "        try:\n",
    "            text1 = text\n",
    "            for j, term in enumerate(list_term):\n",
    "                if term not in list_top20_term:\n",
    "                    text1 = re.sub(term, f'<mark style=\"background-color:yellow\">{term}</mark>', text1, flags=re.I)  \n",
    "                else:\n",
    "                    text1 = re.sub(term, f'<mark style=\"background-color:pink\">{term}</mark>', text1, flags=re.I)  \n",
    "            display(HTML(f'{text1}'))\n",
    "        except:\n",
    "            print(f\"{text} \\n\")\n",
    "    else:\n",
    "        pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2939a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"cluster_name\"] = new_df['cluster'].map(dic_cluster)  \n",
    "new_df[\"origin\"] = domain\n",
    "new_df[\"segm\"] = segment\n",
    "new_df.sort_values(\"cluster\", ascending = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.read_csv(cluster_qualification, sep = \"\\t\").drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beee4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = pd.concat([df_cluster, new_df])\n",
    "new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2.to_csv(f'{path_base}cluster_qualification.csv', sep = \"\\t\") #cluster qualifi√© par origin et segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813bd30",
   "metadata": {},
   "source": [
    "## Facebook\n",
    "\n",
    "### 1√®re p√©riode : 16 novembre 2011 -- 31 janvier 2012:1√®re p√©riode (Segment 0)\n",
    "\n",
    "<img src=\"clustermap/cm_facebook_0.png\" alt=\"segment0_FB\" style=\"height: 100px; width:100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b5973",
   "metadata": {},
   "source": [
    "##### \n",
    "La premi√®re p√©riode sur facebook semble √™tre caract√©ris√©e par trois th√©matiques:\n",
    "\n",
    "- La premi√®re concerne le proc√®s opposant Paul Fran√ßois (c√©r√©alier charantais) √† Monsanto. Paul Fran√ßois s'est retrouv√© dans le coma apr√®s avoir nettoy√© une cuve contenant du Lasso (produit commercialis√© par Monsanto). Le d√©bat a d√©but√© le 12 d√©cembre 2011.\n",
    "\n",
    "> pesticides : un agriculteur en proc√®s contre monsanto Ce lundi 12 d√©cembre 2011, paul fran√ßois, agriculteur victime des pesticides, sera en proc√®s contre monsanto, en premi√®re instance, au TGI de Lyon, dans l'espoir de faire reconna√Ætre la faute de la firme et d'engager la responsabilit√© de cette derni√®re. bioaddict.fr Nous en appelons √† toutes celles et ceux qui le souhaitent √† se rendre au TGI de Lyon le 12 d√©cembre √† 13h30 pour t√©moigner de leur soutien √† paul fran√ßois et, ainsi, √† toutes les victimes des pesticides. [Auteur : BIO ADDICT]\n",
    "\n",
    "- Un deuxi√®me ensemble de textes √©voque le probl√®me du d√©clin des pollinisateurs et, plus sp√©cifiquemet, des abeilles. Les \"termes sp√©cifiques\" du segment 0 que l'on retrouve dans ces textes sont par exemple : *'effet', 'pesticides', 'miel', 'parasite'*. **\"Parasites\"** et **\"pesticides\"** font r√©f√©rence √† deux des causes suppos√©es de disparition des abeilles (les pesticides √† travers les n√©onicotino√Ødes et les parasites avec le Varoa). Une disparition qui se manifesterait notamment par une baisse de production du **miel**.\n",
    "\n",
    "\n",
    "> Fin des abeilles = fin du monde ? Oui bon, c‚Äôest peut-√™tre un peu exag√©r√©. Quoique‚Ä¶sans les abeilles, l‚Äô√©quilibre √©cologique serait gravement menac√©. En effet, les abeilles ont un r√¥le essentiel pour la survie des v√©g√©taux depuis 50 millions d‚Äôann√©es [...] On a d√©couvert que les abeilles sont toxico-sensibles et donc plus vuln√©rables que la plupart des autres insectes aux <mark>pesticides</mark>. [...] En France, le Gaucho de Bayer et le R√©gent de BASF, deux insecticides accus√©s de ravager les abeilles, ont √©t√© interdits en 2006 mais malgr√© tout l‚Äôh√©catombe se poursuit dans les ruches. On ne sait pas bien pourquoi et cela est tr√®s pr√©occupant. <mark>Certains apiculteurs soup√ßonnent que les pesticides</mark> toujours utilis√©s par l‚Äôagriculture sont √† l‚Äôorigine de ce qu‚Äôon appelle d√©sormais le ¬´ syndrome d‚Äôeffondrement des colonies ¬ª, maladie dont on ne sait pas encore grand-chose. [...] R√©cemment, <mark>d‚Äôautres explications</mark> ont √©t√© donn√©es quant √† la disparition des abeilles : \n",
    "> - <mark>Le frelon asiatique</mark> est montr√© du doigt. Cet insecte venu de l‚Äô√©tranger est un pr√©dateur f√©roce pour les abeilles puisqu‚Äôil se nourrit de leurs larves. \n",
    "> - <mark>Une mouche parasite</mark> est √©galement mise en cause depuis peu. Une √©tude am√©ricaine publi√©e en janvier 2012 a en effet mis en lumi√®re le r√¥le n√©faste de la mouche Apocephalus borealis. [Auteur: Let's save our Earth - Sauvons notre plan√®te]\n",
    "\n",
    "- Un troisi√®me ensemble de textes porte sur le rapport publi√© par G√©n√©ration futures sur les  r√©sidus de pesticides dans l'alimentation\n",
    "\n",
    "> Choisir de manger bio, c'est prot√©ger sa sant√© L'association g√©n√©rations futures a effectu√© des tests comparant un menu conventionnel et un menu bio : le r√©sultat est sans appel, le menu bio ne contenant quasiment aucun r√©sidus de pesticides. \"Il y a 223 fois moins de r√©sidus de pesticides en moyenne dans les aliments bio analys√©s que dans les a... bioaddict.fr Mangez bio, c'est bon pour la sant√© ! Et √ßa, c'est vachement bien ! [Auteur : Les 2 vaches, industrie agroalimentaire]\n",
    "\n",
    "- On trouve ensuite les textes qui prennent pour cible \"Monsanto et son monde\" ainsi que les effets sanitaires des produits commercialis√©s par la firme. Il s'agit probablement d'un ensemble de r√©actions √† la suite de la sortie du film √©ponyme *Le monde selon Monsanto*.\n",
    "\n",
    "> *Le monde selon Monsanto* : Ce film retrace l'histoire de monsanto, une multinationale am√©ricaine, aujourd'hui leader mondial des ogm, et consid√©r√©e comme l'un des plus grands pollueurs de l'√®re industrielle (PCB, agent orange, hormones de croissance, roundup...). Apr√®s une enqu√™te de trois ans, en Am√©rique du nord et du sud, en europe et en Asie, il reconstitue la gen√®se d'un empire industriel, qui, √† grand renfort de rapports mensongers, de collusion avec l'administration nord-am√©ricaine, de pressions et tentatives de corruption, est devenu l'un des premiers semenciers de la plan√®te.\n",
    "\n",
    "Il est int√©ressant de noter que Monsanto (et d'autres firmes comme Syngenta ou Bayer) semble √™tre le lien entre diff√©rents \"cluster\": l'affaire Paul Fran√ßois, les d√©bats li√©s au Roundup.\n",
    "\n",
    "- Enfin, de nombreux posts √©voquent les \"pesticides\" de fa√ßon g√©n√©rale sans faire de lien √† une affaire en particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22cfed",
   "metadata": {},
   "source": [
    "## Test (annexe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df2['text'].loc[(df2[\"doc_id\"] == \"413916\")]\n",
    "for x in text :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Sant√© Canada re√ßoit l'ordre de r√©examiner les effets environnementaux du Roundup | equiterre.org \n",
    "- P La Cour f√©d√©rale du Canada a ordonn√© √† Sant√© Canada de r√©√©valuer sa d√©cision de ne pas examiner les effets \n",
    "d'un herbicide, commercialis√© sous le nom de Roundup. Ce pesticide est un des plus populaires sur le march√© et \n",
    "il accompagne une proportion importante des plantes modifi√©es g√©n√©tiquement utilis... equiterre.org \n",
    "Sant√© Canada re√ßoit l'ordre de la Cour f√©d√©rale de r√©examiner les effets environnementaux du RoundUp!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5163c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_term = [\"pesticides\", \"roundup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e40a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for j, term in enumerate(list_term):\n",
    "    if term[-1]== \"s\":\n",
    "        singular_term = term[0:-1]\n",
    "    else:\n",
    "        singular_term = term\n",
    "    #text = text.replace(term[0:len(term)-1], f'<mark style=\"background-color:yellow\">{term}</mark>')\n",
    "    text = re.sub(term, f'<mark style=\"background-color:yellow\">{term}</mark>', text, flags=re.I)          #\n",
    "\n",
    "display(HTML(f'{text}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61d4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyse_tweets",
   "language": "python",
   "name": "analyse_tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
