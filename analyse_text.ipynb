{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bamboolib\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FixedFormatter, FixedLocator\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from IPython.core.display import display, HTML\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7190f",
   "metadata": {},
   "source": [
    "## Analyse des segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aymeric =  \"/home/aymeric/python-scripts/anses_medialab/datas/\" #aymeric\n",
    "jp = '~/Dropbox/Mac/Desktop/CRD Anses/all3/' # Jean Philippe\n",
    "jp_index = '~/Dropbox/Mac/Desktop/CRD Anses/code/indexation_results/' # Jean Philippe index\n",
    "\n",
    "if 'aymeric' in current_directory:\n",
    "    path_base = aymeric\n",
    "    index=f\"{path_base}index_allall_domainsexhaustive.csv\"\n",
    "elif 'Mac' in current_directory:\n",
    "    path_base = jp\n",
    "    index=f'{jp_index}index_allall_domainsexhaustive.csv'\n",
    "elif 'd:/Projects' in current_directory:\n",
    "    path_base = \"d:/Projects/Medialab/\"\n",
    "    index=f\"{path_base}index_allall_domainsexhaustive.csv\"\n",
    "\n",
    "file_segmentation = f\"{path_base}segmentation_common_freq.csv\"\n",
    "data_file = path_base+'fb_tw_med_data.csv.gz'#,line_terminator='\\n',index=False)\n",
    "\n",
    "\n",
    "all_corpus = f\"{path_base}all_corpus_seg_with_sdhi_PI.csv\"\n",
    "cluster_qualification = f\"{path_base}cluster_qualification.csv\"\n",
    "\n",
    "#twittos = f\"{path_base}tweets_pesticides/500_first_twittos_on_pesticides.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_base = [\"index\", \"all_corpus\"]\n",
    "which_base = list_base[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfseg0 = pd.read_csv(file_segmentation, sep = \"\\t\")\n",
    "df_cluster = pd.read_csv(cluster_qualification, sep = \"\\t\")\n",
    "\n",
    "if which_base == \"all_corpus\":\n",
    "    df0 = pd.read_csv(all_corpus, sep = \"\\t\")\n",
    "else:\n",
    "    df_index = pd.read_csv(index, sep = \"\\t\")\n",
    "\n",
    "    df_index['date'] = pd.to_datetime(df_index['date'], infer_datetime_format=True)\n",
    "    df_index['yearmonth']=(df_index['date'].dt.strftime('%Y-%m'))\n",
    "    df_index['date'] = df_index['date'].dt.date\n",
    "    \n",
    "    segment = dfseg0[[\"yearmonth\", \"segm\", \"origin\"]]\n",
    "    df0 = df_index.merge(segment, how = \"inner\", on = [\"origin\", \"yearmonth\"])\n",
    "    df0[\"start_segment\"] = df0.groupby(['origin','segm'])[\"date\"].transform('min')\n",
    "    df0[\"end_segment\"] = df0.groupby(['origin','segm'])[\"date\"].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b866741",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Rename column\n",
    "dft = dft.rename(columns={'id': 'doc_id'})\n",
    "\n",
    "dft['date'] = pd.to_datetime(dft['date'], infer_datetime_format=True)\n",
    "\n",
    "dft['yearmonth']=(dft['date'].dt.strftime('%Y-%m'))\n",
    "dft['date'] = dft['date'].dt.date\n",
    "    \n",
    "segment = dfseg0[[\"yearmonth\", \"segm\", \"origin\"]]\n",
    "dft0 = dft.merge(segment, how = \"inner\", on = [\"origin\", \"yearmonth\"])\n",
    "dft0[\"start_segment\"] = dft0.groupby(['origin','segm'])[\"date\"].transform('min')\n",
    "dft0[\"end_segment\"] = dft0.groupby(['origin','segm'])[\"date\"].transform('max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfseg0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010b9f3",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62530b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, domain, segment):\n",
    "    print(domain)\n",
    "    dfseg = data[0]\n",
    "    dfseg1 = dfseg.loc[(dfseg[\"origin\"] == domain) & (dfseg[\"segm\"] == segment)]\n",
    "    \n",
    "    df = data[1]\n",
    "    df1 = df.loc[(df[\"origin\"] == domain) & (df[\"segm\"] == segment)]\n",
    "    \n",
    "    dft = data[2]\n",
    "    dft1 = dft.loc[(dft[\"origin\"] == domain) & (dft[\"segm\"] == segment)]\n",
    "    dft1['doc_id'] = dft1['doc_id'].astype('string')\n",
    "    \n",
    "    specific_terms = [x.strip() for x in dfseg1[\"top50_term_chi2_True\"].iloc[0].split(\",\")]\n",
    "\n",
    "    print(specific_terms)\n",
    "    df1.loc[df1[\"term\"].isin(specific_terms) == True, \"in_top50_term\"] = 1\n",
    "    df1.loc[df1[\"term\"].isin(specific_terms) == False, \"in_top50_term\"] = 0\n",
    "\n",
    "    \n",
    "    df2 = df1.drop_duplicates(subset = [\"doc_id\", \"term\"]).groupby(['doc_id', 'source', \"date\"]).agg(sum_of_top50_term =('in_top50_term', 'sum')).reset_index()\n",
    "    df2['doc_id'] = df2['doc_id'].astype('string')\n",
    "    df2 = df2.merge(dft1[[\"doc_id\", \"text\"]], how = \"inner\", on = [\"doc_id\"])\n",
    "    \n",
    "    return dfseg1, df1, dft1, df2, specific_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"facebook\", \"twitter\", \"media\"]\n",
    "segment = 1\n",
    "domain = domains[0]\n",
    "\n",
    "list_df = [dfseg0, df0, dft0]\n",
    "dfseg1, df1, dft1, df2, specific_terms = filter_data(list_df, domain = domain, segment = segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f64ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne = np.mean(df2[\"sum_of_top50_term\"]) \n",
    "df3 = df2.loc[df2[\"sum_of_top50_term\"] >= 1 ]\n",
    "df3 = df3.sort_values(by=['date'], ascending=[True])\n",
    "\n",
    "#df3 = df3.drop_duplicates(subset = \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd94bd",
   "metadata": {},
   "source": [
    "## Matrice de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415de650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = df1.groupby(['term', 'doc_id']).agg(count_doc = (\"doc_id\", \"count\")).reset_index()\n",
    "df11=df11.merge(df1[['term','in_top50_term']].drop_duplicates(), on='term', how = 'left')\n",
    "df11=df11.loc[df11[\"in_top50_term\"] ==  1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dico_freq={}\n",
    "for l in df11.groupby('term'):\n",
    "    term=l[0]\n",
    "    #print (term)\n",
    "    dico_freq[term]={}\n",
    "    for d in l[1].groupby('doc_id'):\n",
    "        #print (d[0],d[1]['count_doc'].values[0])\n",
    "        dico_freq[term][d[0]]=d[1]['count_doc'].values[0]\n",
    "\n",
    "for t in dico_freq:\n",
    "    N=sum(dico_freq[t].values())\n",
    "    for d in dico_freq[t]:\n",
    "        dico_freq[t][d]=dico_freq[t][d]/N\n",
    "    \n",
    "                \n",
    "squared={}\n",
    "spec_words=list(dico_freq.keys())\n",
    "for w1 in spec_words:\n",
    "    s=0\n",
    "    for d1 in dico_freq[w1]:\n",
    "        #if d1 in spec_words[w2]:\n",
    "        s += dico_freq[w1][d1] * dico_freq[w1].get(d1,0)\n",
    "    squared[w1]=s\n",
    "\n",
    "similarity=[]\n",
    "spec_words=list(dico_freq.keys())\n",
    "for w1 in spec_words:\n",
    "    sim=[]\n",
    "    for w2 in spec_words:\n",
    "        s=0\n",
    "        for d1 in dico_freq[w1]:\n",
    "            #if d1 in spec_words[w2]:\n",
    "            s += dico_freq[w1][d1] * dico_freq[w2].get(d1,0)\n",
    "        sim.append(s/math.sqrt(squared[w1]*squared[w2]))\n",
    "    similarity.append(sim)\n",
    "\n",
    "#similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = [x for x in df11.term.unique()]\n",
    "print(list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b174a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "cmap_reversed = plt.cm.get_cmap('spring_r')\n",
    "#sns.clustermap(rho_terms)\n",
    "ax=sns.clustermap(similarity,figsize=(15,15), cmap = cmap_reversed, linewidths=2, linecolor='white', vmax = 0.8)#,labels=range(14))\n",
    "new_labels=[]\n",
    "labels_list= list_name\n",
    "for l in ax.ax_heatmap.axes.get_xticklabels():\n",
    "    l.set_text(labels_list[int(l.get_text())])\n",
    "    new_labels.append(l)\n",
    "ax.ax_heatmap.axes.set_yticklabels(new_labels,rotation=0)\n",
    "ax.ax_heatmap.axes.set_xticklabels(new_labels,rotation = 90)\n",
    "\n",
    "#ax.setp(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"clustermap/cm_{domain}_{segment}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211f3c1",
   "metadata": {},
   "source": [
    "## Détermination du nombre de clusters avec kneed Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(15, 5))\n",
    "row_linkage = linkage(sp.distance.pdist(similarity), method='complete')# for x in (t.values, rho_terms.values.T))\n",
    "nbclust_max=20\n",
    "dfcost=pd.DataFrame()\n",
    "for nbclust in  range(1,nbclust_max):\n",
    "    cluster_idx = scipy.cluster.hierarchy.fcluster(row_linkage,nbclust,criterion='maxclust')\n",
    "    cluster = [y for y in cluster_idx]    \n",
    "    new_df = pd.DataFrame(data = {\"term\": list_name, \"cluster\": cluster})\n",
    "    new_df['nbseg']=nbclust\n",
    "    new_df_count = new_df.groupby(['cluster']).agg(term_size=('term', 'size')).reset_index()\n",
    "    new_df_count = new_df_count.loc[new_df_count['term_size'] == 1]\n",
    "    if len(new_df_count)>0:\n",
    "        break\n",
    "        \n",
    "    dfcost=dfcost.append(new_df)\n",
    "    \n",
    "#Liste des distances entre chaque paire de termes\n",
    "df_sim['v1']=df_sim.index\n",
    "df_sim_melt = df_sim.melt(id_vars=['v1'])\n",
    "df_sim_melt.columns=['v1', 'v2', 'sim']\n",
    "df_sim_melt['dist']=1-df_sim_melt['sim']\n",
    "df_sim_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90979c2b",
   "metadata": {},
   "source": [
    "## Distance moyenne entre éléments de chaque cluster (avec nombre de cluster compris entre 1 et 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_clust = pd.merge(df_sim_melt, dfcost, how='inner', left_on=['v1'], right_on=['term'])\n",
    "df_dist_clust = pd.merge(df_dist_clust, dfcost, how='inner', left_on=['v2'], right_on=['term'])\n",
    "df_dist_clust = df_dist_clust.loc[df_dist_clust['nbseg_x'] == df_dist_clust['nbseg_y']]\n",
    "df_dist_clust = df_dist_clust.loc[df_dist_clust['cluster_x'] == df_dist_clust['cluster_y']]\n",
    "#df_dist_clust['distsq']=df_dist_clust['dist']*df_dist_clust['dist']\n",
    "df_dist_clust_inertia = df_dist_clust.groupby(['nbseg_x', 'cluster_x' ,'v1']).agg(dist_mean=('dist', 'mean')).reset_index()\n",
    "df_dist_clust_inertia['distsq']=df_dist_clust_inertia['dist_mean'].pow(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e69cf0",
   "metadata": {},
   "source": [
    "## Somme des distances moyennes au carré intra-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279569be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_clust_inertia = df_dist_clust_inertia.groupby(['nbseg_x', 'cluster_x']).agg(dist_mean_sum=('distsq', 'sum')).reset_index()\n",
    "df_dist_clust_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb48d01",
   "metadata": {},
   "source": [
    "## Somme des distances moyennes au carré inter-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_clust_inertia = df_dist_clust_inertia.groupby(['nbseg_x']).agg(dist_mean_sum=('dist_mean_sum', 'sum')).reset_index()\n",
    "df_dist_clust_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553667fe",
   "metadata": {},
   "source": [
    "## Distribution de dist_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(df_dist_clust_inertia, x='nbseg_x', y='dist_mean_sum', width=800, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15827b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "x=df_dist_clust_inertia['nbseg_x'].values\n",
    "y=df_dist_clust_inertia['dist_mean_sum'].values\n",
    "kn = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "nb_segm_optimal=kn.knee\n",
    "print(nb_segm_optimal)\n",
    "\n",
    "dfcost = dfcost.loc[dfcost['nbseg'] == nb_segm_optimal]\n",
    "dfcost = dfcost.sort_values(by=['cluster'], ascending=[True])\n",
    "\n",
    "dfcost = dfcost.loc[dfcost['nbseg'] == nb_segm_optimal]\n",
    "dfcost = dfcost.sort_values(by=['cluster'], ascending=[True])\n",
    "dfcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp, scipy.cluster.hierarchy as hc\n",
    "\n",
    "fig = plt.figure(1, figsize=(10, 20))\n",
    "#th = 1.63\n",
    "row_linkage = linkage(sp.distance.pdist(similarity), method='complete')# for x in (t.values, rho_terms.values.T))\n",
    "dn = dendrogram(row_linkage, leaf_font_size= 12, labels=list_name, orientation = \"left\")\n",
    "#plt.axvline(x=th, c='grey', lw=1, linestyle='dashed')\n",
    "\n",
    "cluster_idx = scipy.cluster.hierarchy.fcluster(row_linkage,t= th,criterion=\"distance\")\n",
    "plt.savefig(f\"clustermap/dendogram_{domain}_{segment}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae48b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster = [y for y in cluster_idx]\n",
    "    \n",
    "new_df = pd.DataFrame(data = {\"term\": list_name, \"cluster\": cluster})\n",
    "new_df.sort_values(\"cluster\", ascending = True)\n",
    "\n",
    "dic_cluster = {1 : \"Alternatives aux pesticides\", \n",
    "               2 : \"Contre le monde de Monsanto\",  \n",
    "               3 : \"Médecins\", \n",
    "               4 :\"Résidus chimiques\",\n",
    "               5 : \"Expertise et risque sanitaire\",\n",
    "               6 : \"Les systèmes agricoles en question\",\n",
    "               7 : \"Abeilles et néonicotinoïdes\",\n",
    "               8 : \"OGM et herbicides\",\n",
    "               9 : \"Générations futures\",\n",
    "               10 : \"Les résidus de pesticides dans l'alimentation\"\n",
    "              }\n",
    "\n",
    "print(new_df.sort_values(\"cluster\", ascending = True))\n",
    "#print(len(dic_cluster), len(cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0353e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nombre_texts = len(df3[\"text\"])\n",
    "start_segment = df1[\"start_segment\"].iloc[1]#.strftime(\"%Y-%m-%d\")\n",
    "end_segment = df1[\"end_segment\"].iloc[1]#.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for i, text in enumerate(df3[\"text\"]):\n",
    "    doc_id = df3[\"doc_id\"].iloc[i]\n",
    "    list_term = [x for x in df1[\"term\"].loc[(df1[\"doc_id\"] == doc_id)].unique()]\n",
    "    term_to_search = ['victimes']\n",
    "    nb_term_to_search = [x for x in term_to_search if x in list_term]\n",
    "    \n",
    "    if len(nb_term_to_search) >= 0:\n",
    "        #Thematique = \"Tous contre Monsanto\"\n",
    "        #df1.loc[(df1[\"doc_id\"].str.contains(doc_id) == True), \"Subject\"] = Thematique\n",
    "        list_top50_term = [x for x in df1[\"term\"].loc[(df1[\"doc_id\"] == doc_id) & (df1[\"in_top50_term\"] == 1)].unique()]\n",
    "        list_top50_term_absent = set(specific_terms) - set(list_top50_term)\n",
    "        date = df3[\"date\"].iloc[i]#.strftime(\"%Y-%m-%d\")\n",
    "        auteur = df3[\"source\"].iloc[i]\n",
    "        display(HTML(f\"<h2 style='text-align:center;'> Texte {doc_id} ({i+1} sur {nombre_texts})</h2>\"))\n",
    "        display(HTML(f\"<strong>Segment :</strong> {segment} ({start_segment} ; {end_segment})\"))\n",
    "        display(HTML(f\"<strong>Date :</strong> {date}\"))\n",
    "        display(HTML(f\"<strong>Auteur :</strong> {auteur} \\n\"))\n",
    "        print(f\"Termes indexés : {list_term} \\n\")\n",
    "        print(f\"Termes spécifiques présents : {list_top50_term} \\n\")\n",
    "        print(f\"Termes spécifiques absents : {list_top50_term_absent} \\n\")\n",
    "        #display(HTML(f\"<strong>Thématique :</strong> {Thematique} \\n\"))\n",
    "        try:\n",
    "            text1 = text\n",
    "            for j, term in enumerate(list_term):\n",
    "                if term not in list_top20_term:\n",
    "                    text1 = re.sub(term, f'<mark style=\"background-color:yellow\">{term}</mark>', text1, flags=re.I)  \n",
    "                else:\n",
    "                    text1 = re.sub(term, f'<mark style=\"background-color:pink\">{term}</mark>', text1, flags=re.I)  \n",
    "            display(HTML(f'{text1}'))\n",
    "        except:\n",
    "            print(f\"{text} \\n\")\n",
    "    else:\n",
    "        pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2939a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"cluster_name\"] = new_df['cluster'].map(dic_cluster)  \n",
    "new_df[\"origin\"] = domain\n",
    "new_df[\"segm\"] = segment\n",
    "new_df.sort_values(\"cluster\", ascending = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.read_csv(cluster_qualification, sep = \"\\t\").drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beee4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = pd.concat([df_cluster, new_df])\n",
    "new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2.to_csv(f'{path_base}cluster_qualification.csv', sep = \"\\t\") #cluster qualifié par origin et segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813bd30",
   "metadata": {},
   "source": [
    "## Facebook\n",
    "\n",
    "### 1ère période : 16 novembre 2011 -- 31 janvier 2012:1ère période (Segment 0)\n",
    "\n",
    "<img src=\"clustermap/cm_facebook_0.png\" alt=\"segment0_FB\" style=\"height: 100px; width:100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b5973",
   "metadata": {},
   "source": [
    "##### \n",
    "La première période sur facebook semble être caractérisée par trois thématiques:\n",
    "\n",
    "- La première concerne le procès opposant Paul François (céréalier charantais) à Monsanto. Paul François s'est retrouvé dans le coma après avoir nettoyé une cuve contenant du Lasso (produit commercialisé par Monsanto). Le débat a débuté le 12 décembre 2011.\n",
    "\n",
    "> pesticides : un agriculteur en procès contre monsanto Ce lundi 12 décembre 2011, paul françois, agriculteur victime des pesticides, sera en procès contre monsanto, en première instance, au TGI de Lyon, dans l'espoir de faire reconnaître la faute de la firme et d'engager la responsabilité de cette dernière. bioaddict.fr Nous en appelons à toutes celles et ceux qui le souhaitent à se rendre au TGI de Lyon le 12 décembre à 13h30 pour témoigner de leur soutien à paul françois et, ainsi, à toutes les victimes des pesticides. [Auteur : BIO ADDICT]\n",
    "\n",
    "- Un deuxième ensemble de textes évoque le problème du déclin des pollinisateurs et, plus spécifiquemet, des abeilles. Les \"termes spécifiques\" du segment 0 que l'on retrouve dans ces textes sont par exemple : *'effet', 'pesticides', 'miel', 'parasite'*. **\"Parasites\"** et **\"pesticides\"** font référence à deux des causes supposées de disparition des abeilles (les pesticides à travers les néonicotinoïdes et les parasites avec le Varoa). Une disparition qui se manifesterait notamment par une baisse de production du **miel**.\n",
    "\n",
    "\n",
    "> Fin des abeilles = fin du monde ? Oui bon, c’est peut-être un peu exagéré. Quoique…sans les abeilles, l’équilibre écologique serait gravement menacé. En effet, les abeilles ont un rôle essentiel pour la survie des végétaux depuis 50 millions d’années [...] On a découvert que les abeilles sont toxico-sensibles et donc plus vulnérables que la plupart des autres insectes aux <mark>pesticides</mark>. [...] En France, le Gaucho de Bayer et le Régent de BASF, deux insecticides accusés de ravager les abeilles, ont été interdits en 2006 mais malgré tout l’hécatombe se poursuit dans les ruches. On ne sait pas bien pourquoi et cela est très préoccupant. <mark>Certains apiculteurs soupçonnent que les pesticides</mark> toujours utilisés par l’agriculture sont à l’origine de ce qu’on appelle désormais le « syndrome d’effondrement des colonies », maladie dont on ne sait pas encore grand-chose. [...] Récemment, <mark>d’autres explications</mark> ont été données quant à la disparition des abeilles : \n",
    "> - <mark>Le frelon asiatique</mark> est montré du doigt. Cet insecte venu de l’étranger est un prédateur féroce pour les abeilles puisqu’il se nourrit de leurs larves. \n",
    "> - <mark>Une mouche parasite</mark> est également mise en cause depuis peu. Une étude américaine publiée en janvier 2012 a en effet mis en lumière le rôle néfaste de la mouche Apocephalus borealis. [Auteur: Let's save our Earth - Sauvons notre planète]\n",
    "\n",
    "- Un troisième ensemble de textes porte sur le rapport publié par Génération futures sur les  résidus de pesticides dans l'alimentation\n",
    "\n",
    "> Choisir de manger bio, c'est protéger sa santé L'association générations futures a effectué des tests comparant un menu conventionnel et un menu bio : le résultat est sans appel, le menu bio ne contenant quasiment aucun résidus de pesticides. \"Il y a 223 fois moins de résidus de pesticides en moyenne dans les aliments bio analysés que dans les a... bioaddict.fr Mangez bio, c'est bon pour la santé ! Et ça, c'est vachement bien ! [Auteur : Les 2 vaches, industrie agroalimentaire]\n",
    "\n",
    "- On trouve ensuite les textes qui prennent pour cible \"Monsanto et son monde\" ainsi que les effets sanitaires des produits commercialisés par la firme. Il s'agit probablement d'un ensemble de réactions à la suite de la sortie du film éponyme *Le monde selon Monsanto*.\n",
    "\n",
    "> *Le monde selon Monsanto* : Ce film retrace l'histoire de monsanto, une multinationale américaine, aujourd'hui leader mondial des ogm, et considérée comme l'un des plus grands pollueurs de l'ère industrielle (PCB, agent orange, hormones de croissance, roundup...). Après une enquête de trois ans, en Amérique du nord et du sud, en europe et en Asie, il reconstitue la genèse d'un empire industriel, qui, à grand renfort de rapports mensongers, de collusion avec l'administration nord-américaine, de pressions et tentatives de corruption, est devenu l'un des premiers semenciers de la planète.\n",
    "\n",
    "Il est intéressant de noter que Monsanto (et d'autres firmes comme Syngenta ou Bayer) semble être le lien entre différents \"cluster\": l'affaire Paul François, les débats liés au Roundup.\n",
    "\n",
    "- Enfin, de nombreux posts évoquent les \"pesticides\" de façon générale sans faire de lien à une affaire en particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22cfed",
   "metadata": {},
   "source": [
    "## Test (annexe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df2['text'].loc[(df2[\"doc_id\"] == \"413916\")]\n",
    "for x in text :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Santé Canada reçoit l'ordre de réexaminer les effets environnementaux du Roundup | equiterre.org \n",
    "- P La Cour fédérale du Canada a ordonné à Santé Canada de réévaluer sa décision de ne pas examiner les effets \n",
    "d'un herbicide, commercialisé sous le nom de Roundup. Ce pesticide est un des plus populaires sur le marché et \n",
    "il accompagne une proportion importante des plantes modifiées génétiquement utilis... equiterre.org \n",
    "Santé Canada reçoit l'ordre de la Cour fédérale de réexaminer les effets environnementaux du RoundUp!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5163c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_term = [\"pesticides\", \"roundup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e40a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for j, term in enumerate(list_term):\n",
    "    if term[-1]== \"s\":\n",
    "        singular_term = term[0:-1]\n",
    "    else:\n",
    "        singular_term = term\n",
    "    #text = text.replace(term[0:len(term)-1], f'<mark style=\"background-color:yellow\">{term}</mark>')\n",
    "    text = re.sub(term, f'<mark style=\"background-color:yellow\">{term}</mark>', text, flags=re.I)          #\n",
    "\n",
    "display(HTML(f'{text}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61d4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyse_tweets",
   "language": "python",
   "name": "analyse_tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
